#!/bin/bash

PALAVRAS=(hadoop mapreduce yarn hdfs datanode namenode distributed computing big data processing cluster parallel framework apache java)

# DiretÃ³rio onde os dados serÃ£o gerados (fora do Docker)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
MASSA_DIR="$SCRIPT_DIR/../massa_de_dados"

mkdir -p "$MASSA_DIR"

OUTPUT="$MASSA_DIR/massa_unica.txt"

# Limpa o arquivo caso jÃ¡ exista
> "$OUTPUT"

gera_linhas() {
    local count=$1
    local repeticoes=5000  # tamanho do bloco
    for ((k=0; k<count; k+=repeticoes)); do
        linha=""
        for ((j=0;j<15;j++)); do
            linha+=" ${PALAVRAS[$RANDOM % ${#PALAVRAS[@]}]}"
        done
        yes "$linha" | head -n $repeticoes
    done
}

# VersÃ£o mais rÃ¡pida: 30 arquivos equivalentes ao invÃ©s de 80
# Isso mantÃ©m o job rodando por tempo suficiente (2-3 min) para os testes
TOTAL_LINHAS=$((1000000 * 30))

echo "Gerando massa de dados rÃ¡pida para testes (~30 arquivos equivalentes)..."
echo "Tempo estimado: 30-60 segundos"

# Gera e adiciona ao arquivo final
gera_linhas "$TOTAL_LINHAS" >> "$OUTPUT"

TAMANHO=$(du -h "$OUTPUT" | cut -f1)
echo "âœ… Arquivo gerado em: $OUTPUT"
echo "ðŸ“¦ Tamanho: $TAMANHO"
echo ""
echo "âš¡ Essa versÃ£o Ã© mais rÃ¡pida mas mantÃ©m os jobs com duraÃ§Ã£o razoÃ¡vel para testes"
echo "ðŸ’¡ Para testes de performance completos, use: ./gerar_dados.sh"
